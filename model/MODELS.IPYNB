{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Assignment 2 — Model Evaluation Notebook\n",
    "\n",
    "This notebook trains all **6 required classification models** on the **Breast Cancer Wisconsin (Diagnostic)** dataset and produces:\n",
    "\n",
    "- Accuracy\n",
    "- AUC\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- MCC\n",
    "- A Markdown comparison table ready to paste into README.md\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from model.data import load_dataset, split_data, make_scaled\n",
    "from model.evaluate import compute_metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, feature_names, target_names = load_dataset()\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models\n",
    "Same configuration used in the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": make_scaled(LogisticRegression(max_iter=500)),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    \"KNN\": make_scaled(KNeighborsClassifier(n_neighbors=5)),\n",
    "    \"Naive Bayes (Gaussian)\": GaussianNB(),\n",
    "    \"Naive Bayes (Multinomial)\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        eval_metric=\"logloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1] if hasattr(model, \"predict_proba\") else None\n",
    "    m = compute_metrics(y_test, y_pred, y_proba)\n",
    "    row = {\"ML Model Name\": name}\n",
    "    row.update(m)\n",
    "    results.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate README-Friendly Markdown Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| ML Model Name | Accuracy | AUC | Precision | Recall | F1 | MCC |\\n|---|---:|---:|---:|---:|---:|---:|\\n| Logistic Regression | 0.9649 | 0.9938 | 0.9649 | 0.9649 | 0.9649 | 0.9297 |\\n| Decision Tree | 0.9123 | 0.9217 | 0.9217 | 0.9123 | 0.9123 | 0.8258 |\\n| KNN | 0.9561 | 0.9842 | 0.9561 | 0.9561 | 0.9561 | 0.9105 |\\n| Naive Bayes (Gaussian) | 0.9386 | 0.9764 | 0.9386 | 0.9386 | 0.9386 | 0.8777 |\\n| Naive Bayes (Multinomial) | 0.8947 | 0.9351 | 0.8947 | 0.8947 | 0.8947 | 0.7897 |\\n| Random Forest | 0.9737 | 0.9928 | 0.9737 | 0.9737 | 0.9737 | 0.9473 |\\n| XGBoost | 0.9737 | 0.9971 | 0.9737 | 0.9737 | 0.9737 | 0.9473 |'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = \"| ML Model Name | Accuracy | AUC | Precision | Recall | F1 | MCC |\\n\"\n",
    "md += \"|---|---:|---:|---:|---:|---:|---:|\\n\"\n",
    "\n",
    "for _, r in df_results.iterrows():\n",
    "    def fmt(x): return \"-\" if pd.isna(x) else f\"{x:.4f}\"\n",
    "    md += f\"| {r['ML Model Name']} | {fmt(r['accuracy'])} | {fmt(r.get('roc_auc'))} | {fmt(r['precision'])} | {fmt(r['recall'])} | {fmt(r['f1'])} | {fmt(r['mcc'])} |\\n\"\n",
    "\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the above Markdown table into your `README.md`\n",
    "\n",
    "This satisfies the assignment’s requirement for presenting a comparison table across all 6 models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
